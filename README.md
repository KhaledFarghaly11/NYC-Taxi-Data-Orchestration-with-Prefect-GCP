# NYC Taxi Data Pipeline

This project consists of an Extract, Transform, and Load (ETL) pipeline that processes New York City taxi data, using Pandas for data manipulation, Prefect for workflow orchestration, and Google Cloud Platform (GCP) for storage and data warehousing. The pipeline fetches the data, cleans it, and uploads the results to Google Cloud Storage (GCS) and BigQuery for further analysis.

## Description

The main workflow consists of several tasks organized into two Prefect flows:
- `etl_web_to_gcs`: Extracts taxi data from a web source, performs cleaning operations, saves the data locally as a Parquet file, and then uploads it to GCS.
- `etl_parent_flow`: A higher-level flow that iteratively calls `etl_web_to_gcs` for multiple months of data.
- `etl_gcs_to_bq`: Pulls the cleaned Parquet files from GCS, performs further transformation, and then loads the data into BigQuery for analysis.

## Features

- Data extraction from public web sources
- Data cleaning and transformation using Pandas
- Local storage of data as compressed Parquet files
- Data upload to GCS and BigQuery
- Prefect task retries and result caching
- GCP authentication and resource handling

## Setup

1. Clone the repository to your local machine or cloud environment.
2. Install the required Python libraries, including `pandas`, `prefect`, and GCP-related packages as specified in `requirements.txt`.
3. Set up GCP credentials and necessary GCS and BigQuery resources.
4. Configure Prefect with your desired scheduling and execution preferences.
5. Run the main script to execute the pipeline.

## Usage

To execute the entire pipeline, run:

```python
python flows/etl_web_to_gcs.py
python flows/etl_gcs_to_bq.py

## Contributing

We welcome contributions that improve the efficiency and usability of the pipeline. Please follow the standard fork-branch-pull request workflow.

## Troubleshooting

If you encounter any issues or errors while running the pipeline, here are some common troubleshooting steps:

1. Double-check your GCP credentials and ensure that they have the necessary permissions to access the required resources.
2. Verify that the web source for the taxi data is still accessible and providing the expected data format.
3. Check the logs and error messages generated by Prefect and the GCP services for any specific error details.
4. Make sure that you have installed all the required Python libraries and their compatible versions as specified in `requirements.txt`.
5. If you are experiencing performance issues, consider optimizing the data processing steps or scaling up the resources in your GCP environment.

## Support

If you need any assistance or have any questions about the NYC Taxi Data Pipeline, please feel free to reach out.

## Acknowledgements

I would like to thank the contributors and maintainers of the open-source libraries and frameworks used in this project, including Pandas, Prefect, Google Cloud Platform, and others. Their efforts and dedication have made this pipeline possible.




